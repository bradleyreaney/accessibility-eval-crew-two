### PERSONA

You are an expert digital accessibility consultant. Your expertise is rooted in a deep, practical understanding of the Web Content Accessibility Guidelines (WCAG) 2.1 and 2.2, particularly at conformance levels A and AA. You are proficient in evaluating remediation strategies not just for technical correctness, but for strategic coherence, efficiency, and real-world impact on users with disabilities. You think like a senior developer, a project manager, and a user advocate simultaneously.

### CORE TASK

You will act as an impartial judge to evaluate and compare the accessibility remediation plans provided below. These plans were generated by other AI models based on the findings from an initial accessibility audit report. Your task is to determine which plan is superior and, more importantly, to provide a detailed analysis that can be used to construct a single, optimal remediation strategy.

### CONTEXT: ACCESSIBILITY AUDIT FINDINGS



### CANDIDATE REMEDIATION PLANS

Here are the remediation plans you must evaluate:

#### Plan A:


#### Plan B:


(Add more plans, e.g., Plan C, Plan D, as needed)

### EVALUATION FRAMEWORK & OUTPUT STRUCTURE

You must provide your evaluation in the exact format specified below. Do not deviate from this structure. Use Markdown for all formatting.

---

## Comprehensive Evaluation Report

### 1. Comparative Analysis of Remediation Plans

For each plan (Plan A, Plan B, etc.), provide a detailed assessment based on the following criteria.

#### Evaluation of Plan A

*   **Strategic Prioritization (Weight: 40%)**:
    *   Assess the logic and rationale behind the plan's sequencing of tasks.
    *   Does it effectively synthesize multiple prioritization models (e.g., user impact, architectural leverage, effort, risk)? Or does it naively follow a single, rigid rule?
    *   How well does it prioritize critical user paths and high-impact, site-wide issues?
    *   Provide specific examples of its prioritization strengths and weaknesses.

*   **Technical Specificity & Correctness (Weight: 30%)**:
    *   Evaluate the clarity, accuracy, and actionability of the proposed technical solutions.
    *   Are the solutions specific enough for a developer to implement without further research (e.g., providing code snippets, specific CSS values, correct ARIA roles)?
    *   Are the proposed solutions technically sound and aligned with modern web development best practices?

*   **Comprehensiveness & Structure (Weight: 20%)**:
    *   Does the plan address all violations noted in the audit report?
    *   Is the plan well-structured and easy to understand for a multi-disciplinary team (developers, designers, project managers)?
    *   Does the plan explicitly connect its proposed fixes back to the foundational POUR principles (Perceivable, Operable, Understandable, Robust)?

*   **Long-Term Vision (Weight: 10%)**:
    *   Does the plan include provisions for post-remediation verification and ongoing monitoring?
    *   Does it demonstrate an understanding of accessibility as a continuous process rather than a one-time fix?

#### Evaluation of Plan B

*   **Strategic Prioritization (Weight: 40%)**:
    *   (Repeat the same detailed assessment as for Plan A)
*   **Technical Specificity & Correctness (Weight: 30%)**:
    *   (Repeat the same detailed assessment as for Plan A)
*   **Comprehensiveness & Structure (Weight: 20%)**:
    *   (Repeat the same detailed assessment as for Plan A)
*   **Long-Term Vision (Weight: 10%)**:
    *   (Repeat the same detailed assessment as for Plan A)

(Repeat this evaluation structure for all other candidate plans.)

### 2. Head-to-Head Comparison: Pros and Cons

Present a summary table comparing the plans.

| Feature | Plan A | Plan B |
| :--- | :--- | :--- |
| **Pros** | - List 2-3 key strengths | - List 2-3 key strengths |
| **Cons** | - List 2-3 key weaknesses | - List 2-3 key weaknesses |

### 3. The Ideal Plan: What's Missing?

Based on your expert knowledge, describe the key elements that an ideal, world-class remediation plan would contain that are missing from ALL of the provided plans. This could include more sophisticated prioritization logic, more robust technical solutions for specific problems, a clearer project management framework, or recommendations for team training and cultural change. Be specific and provide actionable examples.

### 4. Final Verdict and Scoring

Conclude with a final verdict. State which plan you consider superior overall and briefly justify your choice based on the weighted criteria. Then, provide a score for each plan.

Render your final scores in a single, clean JSON object at the very end of your response, and nowhere else.

```json
{
  "Plan A": {
    "score": X.X,
    "rationale": "Brief summary of the reasoning for this score."
  },
  "Plan B": {
    "score": Y.Y,
    "rationale": "Brief summary of the reasoning for this score."
  }
}
```

### 2.3. An Annotated Deconstruction of the Master Prompt
The master prompt is a carefully engineered instrument. Each component is designed to elicit a specific type of analysis from the LLM, ensuring the final output is both comprehensive and deeply insightful. The following table deconstructs the prompt to reveal the purpose behind each instruction.

| Prompt Component | Core Instruction | Analytical Purpose | Key Principles Tested | Supporting Research |
| :--- | :--- | :--- | :--- | :--- |
| **### PERSONA** | Assume the role of an expert accessibility consultant. | To prime the model to use specialized, domain-specific knowledge and adopt a professional, authoritative tone. | Expertise, Authority | N/A |
| **### CORE TASK** | Act as a judge to compare plans and inform a final, optimal strategy. | To frame the task not as a simple competition, but as a constructive process aimed at synthesis and improvement. | Strategic Synthesis | N/A |
| **### CONTEXT** | Ingest the original accessibility audit report. | To provide a ground truth against which the completeness and relevance of each plan can be accurately measured. | Comprehensiveness, Relevance | [10, 11] |
| **### EVALUATION: Strategic Prioritization** | Assess the logic for sequencing tasks, looking for a synthesis of models. | To test for the most critical and complex aspect of a remediation plan: its strategic intelligence. This forces the model to look beyond simple lists and evaluate the *why* behind the plan's structure. | Multi-Factor Prioritization, User-Centricity, Architectural Leverage, Risk Management | [14, 15, 17, 19] |
| **### EVALUATION: Technical Specificity** | Evaluate the clarity, correctness, and actionability of the proposed fixes. | To ensure the plan is practical and can be executed by a development team without ambiguity or the need for significant additional research. | Actionability, Technical Validity | [10, 11, 20] |
| **### EVALUATION: Comprehensiveness & Structure** | Check if all audit issues are addressed and if the plan connects fixes to POUR principles. | To assess the plan's thoroughness and its demonstration of a deeper, principle-based understanding of accessibility, rather than just rote compliance. | Foundational Understanding (POUR), Project Management Utility | [1, 6, 21, 22] |
| **### EVALUATION: Long-Term Vision** | Look for provisions for verification and ongoing monitoring. | To differentiate between plans that treat accessibility as a one-off project and those that treat it as a continuous, sustainable process. | Continuous Improvement | [10, 12, 16] |
| **### Ideal Plan: What's Missing?** | Identify elements of an expert plan that are absent from all candidates. | To leverage the LLM's generative capabilities for a gap analysis, highlighting areas for improvement beyond what the initial plans conceived. | Gap Analysis, Best Practices | [16, 20] |
| **### Final Verdict and Scoring (JSON)** | Provide a final comparative judgment and a structured, quantitative score. | To provide a clear, summary conclusion and a machine-readable output that is consistent and easy to process. | Clarity, Consistency, Parsability | N/A |

---

## Section 3: A Strategic Guide to Implementation and Interpretation
Possessing the master prompt is only the first step. The true value of this framework is realized through its effective implementation and, more importantly, the strategic interpretation of its output. This section provides a practical guide for using the "expert judge" LLM not just to pick a winner, but to construct a genuinely superior accessibility remediation strategy.

### 3.1. Executing the Evaluation: A Practical Workflow
Following a structured workflow ensures consistent and reliable results when using the master prompt.

1.  **Prepare Inputs**: Gather the two essential pieces of context: the original accessibility audit report and the candidate remediation plans generated by the LLMs. Ensure they are in a clean, plain text format. Copy and paste this content directly into the corresponding `[---]` placeholders in the master prompt template. Be meticulous; the quality of the judge's analysis is directly dependent on the quality of the input it receives.
2.  **Deploy Prompt**: Submit the completed prompt to the chosen LLM. Be aware that the comprehensive nature of the prompt may result in a lengthy and detailed response. It is advisable to use LLM interfaces that can handle large inputs and outputs without truncation.
3.  **Handle Variations**: Different LLMs may interpret instructions with slight variations or exhibit different strengths. For the most robust assessment, consider running the same prompt through multiple high-capability models. This can provide a more balanced perspective, and areas where all "judge" LLMs agree are likely to be highly reliable findings. For any single model, running the prompt a second time can also help smooth out random variations and confirm the stability of its assessment.

### 3.2. Interpreting the Verdict: From Scores to a Synthesized Strategy
The output from the judge LLM is a rich analytical document. The numerical score is the least valuable component; the true utility lies in the qualitative analysis, which serves as a blueprint for creating a final, optimized plan.

The goal is not to simply choose the plan with the highest score. The goal is to **build a champion plan** by synthesizing the best elements of all candidates and addressing their collective weaknesses.

*   **Synthesize the "Pros"**: Review the "Pros" listed for each plan in the comparative analysis. Create a new, hybrid plan that incorporates the strongest features from each. For example, Plan A might have offered a brilliant, multi-factor prioritization strategy [14, 17], while Plan B provided exceptionally clear, actionable code snippets for fixing complex form-related issues.[11, 23] The synthesized plan should adopt Plan A's prioritization logic and integrate Plan B's technical solutions.
*   **Address the "Cons"**: The "Cons" section for each plan is a targeted list of weaknesses to be rectified. If Plan A's solutions were too generic, replace them with more specific ones. If Plan B failed to address all the issues from the audit, ensure the missing items are added to the synthesized plan.
*   **Incorporate the "What's Missing"**: This section of the judge's report is a critical gap analysis. It highlights what an expert would have included that no AI even considered. This is often where elements related to process and culture emerge. For instance, the judge might note that no plan included a recommendation for training the content team on writing effective alt text [12, 20] or a strategy for integrating automated accessibility checks into the continuous integration/continuous deployment (CI/CD) pipeline. These expert recommendations should be added as new, high-value sections in the final synthesized plan.

By following this process, the user transforms the LLM evaluation from a passive contest into an active, constructive exercise. The LLMs serve as brainstorming partners, and the judge LLM acts as a structured critic, guiding the human user in the final assembly of a comprehensive and strategically sound remediation plan.

### 3.3. Beyond the Judge: The Imperative of Human Oversight and Continuous Compliance
It is imperative to recognize the role of this entire framework within a broader context of accountability and professional responsibility. The LLM, even when acting as an expert judge, is a sophisticated decision-support tool, not a replacement for qualified human expertise. The output of this process should be considered a highly advanced first draft, not a final, deployable strategy.

This "hyper-intern" generated plan must be reviewed, validated, and ultimately approved by a human project lead, senior developer, or accessibility specialist. Humans must verify the technical feasibility of the proposed solutions, adjust priorities based on internal business context, and take ultimate ownership of the compliance effort.

Furthermore, the synthesized plan is not the end of the journey. It is the beginning of a new phase in the continuous cycle of accessibility.[11, 16] True digital inclusion is achieved and maintained through an ongoing commitment that includes:

*   **Regular Audits**: Scheduling periodic accessibility audits, especially after major site redesigns or feature launches, to catch new barriers.[10, 12]
*   **Team Training**: Investing in ongoing training for developers, designers, and content creators to ensure they understand and apply accessibility best practices in their daily work.[20]
*   **Cultural Integration**: Fostering a culture of accessibility where inclusion is not an afterthought or a compliance checkbox, but a core value that is integrated into the entire product development lifecycle from the very beginning.[20]

Ultimately, the framework presented in this report empowers organizations to leverage the speed and scale of AI to tackle the complexities of web accessibility. However, its greatest potential is realized when this technological power is guided by human wisdom, strategic oversight, and an unwavering commitment to creating a digital world that works for everyone.

---

## Conclusion
This report has detailed a robust and systematic framework for leveraging Large Language Models to critically evaluate and enhance web accessibility remediation plans. By establishing a "gold standard" for what constitutes a world-class plan—one that is strategically prioritized, technically specific, and rooted in the foundational POUR principles—we have created a firm basis for objective assessment.

The primary contribution is the master prompt, an engineered tool designed to guide an LLM to act as a discerning expert judge. The annotated deconstruction of this prompt reveals a deliberate methodology, where each instruction is crafted to test for specific hallmarks of quality, from sophisticated, multi-factor prioritization to a long-term, sustainable vision for compliance.

Crucially, this framework advocates for a process that moves beyond simple comparison to active synthesis. The goal is not to identify the best of the initial AI-generated options, but to use the judge's detailed critique to construct a single, superior strategy that amalgamates the strengths and rectifies the weaknesses of all candidates.

The synergistic potential of this approach is significant. It allows teams to harness the generative power of AI to rapidly produce remediation options while using a structured, AI-driven evaluation to impose rigor, identify gaps, and guide the creation of a final, expert-informed plan. However, this entire process must remain under the stewardship of human experts. Technology can accelerate and inform, but accountability, strategic decision-making, and the ultimate commitment to an inclusive digital experience remain fundamentally human responsibilities. By pairing the analytical power of AI with human oversight, organizations can navigate the path to accessibility compliance more effectively and efficiently than ever before.