name: Quality Gates

on:
  push:
    branches: [ main, phase-one, phase-two, develop ]
  pull_request:
    branches: [ main ]

jobs:
  quality-gates:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Increased timeout for dependency resolution
    strategy:
      fail-fast: false  # Don't cancel other jobs if one fails
      matrix:
        python-version: [3.11, 3.12]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # Configure pip for faster, more reliable installation
        pip config set global.timeout 60
        pip config set global.retries 3
        # Use minimal CI dependencies for faster installation
        pip install -r requirements-ci.txt

    - name: Code Quality - Black Formatting Check
      run: |
        black --check --diff src/ tests/
      continue-on-error: false

    - name: Code Quality - Flake8 Linting
      run: |
        flake8 src/ tests/ --statistics
      continue-on-error: false

    - name: Type Checking - mypy
      timeout-minutes: 5  # Prevent mypy hangs
      run: |
        mypy src/ --ignore-missing-imports --no-strict-optional
      continue-on-error: true  # Allow to pass while we improve type coverage

    - name: Security Scan - bandit
      timeout-minutes: 5  # Prevent security scan hangs
      run: |
        pip install bandit
        bandit -r src/ -f json -o bandit-report.json || true
        bandit -r src/ --severity-level medium
      continue-on-error: false

    - name: Unit Tests with Coverage
      timeout-minutes: 10  # Prevent test hangs
      run: |
        python -m pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=term-missing --cov-fail-under=90 --tb=short
      env:
        PYTHONPATH: ${{ github.workspace }}/src

    - name: Upload Coverage to Codecov
      timeout-minutes: 3  # Prevent upload hangs
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false  # Don't fail if codecov is down

    - name: Integration Tests (if API keys available)
      timeout-minutes: 10  # Prevent integration test hangs
      run: |
        if [ -n "${{ secrets.GOOGLE_API_KEY }}" ] && [ -n "${{ secrets.OPENAI_API_KEY }}" ]; then
          python -m pytest tests/integration/ -v --llm --tb=short
        else
          echo "âš ï¸  Integration tests skipped - API keys not configured"
        fi
      env:
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        PYTHONPATH: ${{ github.workspace }}/src

    - name: Performance Benchmarks
      run: |
        python -m pytest tests/unit/ --durations=10 --tb=no -q
        # Fail if any test takes > 5 seconds
        python -c "
        import subprocess
        import sys
        result = subprocess.run(['python', '-m', 'pytest', 'tests/unit/', '--durations=0', '--tb=no', '-q'],
                              capture_output=True, text=True)
        lines = result.stdout.split('\n')
        slow_tests = [l for l in lines if 'PASSED' in l and any(f'{i}.' in l for i in range(5, 100))]
        if slow_tests:
            print('âŒ Tests taking >5s:', slow_tests)
            sys.exit(1)
        print('âœ… All tests under 5s performance threshold')
        "

    - name: Documentation Check
      run: |
        # Check that all modules have docstrings
        python -c "
        import ast
        import sys
        from pathlib import Path

        def check_docstrings(file_path):
            with open(file_path, 'r') as f:
                tree = ast.parse(f.read())

            missing = []
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                    if not ast.get_docstring(node):
                        missing.append(f'{file_path}:{node.lineno} - {node.name}')
            return missing

        all_missing = []
        for py_file in Path('src').rglob('*.py'):
            if py_file.name != '__init__.py':
                missing = check_docstrings(py_file)
                all_missing.extend(missing)

        if all_missing:
            print('âŒ Missing docstrings:')
            for item in all_missing[:10]:  # Show first 10
                print(f'  {item}')
            if len(all_missing) > 10:
                print(f'  ... and {len(all_missing) - 10} more')
            # Don't fail for now, just warn
        else:
            print('âœ… All functions and classes have docstrings')
        "

    - name: Quality Gates Summary
      if: always()
      run: |
        echo "ğŸ¯ Quality Gates Results:"
        echo "This summary shows hardcoded success messages."
        echo "Check individual step results above for actual status."
        echo "Pipeline will fail if any critical step fails."
        echo ""
        if [ "${{ job.status }}" == "success" ]; then
          echo "ğŸš€ All quality gates passed - Ready for deployment!"
        else
          echo "âŒ Some quality gates failed - Review errors above"
        fi
